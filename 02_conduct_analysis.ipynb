{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import igraph as ig\n",
    "import nltk\n",
    "import pickle\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from stop_words import get_stop_words\n",
    "from scipy.stats import spearmanr, ks_2samp\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from svglib.svglib import svg2rlg\n",
    "from reportlab.graphics import renderPDF\n",
    "from wordcloud import WordCloud\n",
    "from stop_words import get_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.proportion import proportions_ztest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_input_path = \"data/texts.p\"\n",
    "sentiment_dir = \"data/sentiment/\" # use / at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "plt.rc('ps',fonttype = 42)\n",
    "plt.rc('pdf',fonttype = 42)\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "plt.rcParams['ps.useafm'] = True\n",
    "plt.rcParams['pdf.use14corefonts'] = True\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = [\"French\", \"German\", \"Italian\", \"Portuguese\", \"Spanish\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_df = pd.read_pickle(dataframe_input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_nlp = sc.load(\"de_core_news_sm\")\n",
    "fr_nlp = sc.load(\"fr_core_news_sm\")\n",
    "es_nlp = sc.load(\"es_core_news_sm\")\n",
    "it_nlp = sc.load(\"it_core_news_sm\")\n",
    "pt_nlp = sc.load(\"pt_core_news_sm\")\n",
    "\n",
    "nlp_to_use = {\n",
    "    \"French\": fr_nlp,\n",
    "    \"German\": de_nlp,\n",
    "    \"Italian\": it_nlp,\n",
    "    \"Portuguese\": pt_nlp,\n",
    "    \"Spanish\": es_nlp\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_lexica = {}\n",
    "for lang in languages:\n",
    "    sentiment_lexica[lang] = {}\n",
    "    with open(\"{}negative_words_{}.txt\".format(sentiment_dir, lang.lower()), \"r\") as fr:\n",
    "        sentiment_lexica[lang][\"neg\"] = fr.read().splitlines()\n",
    "    with open(\"{}positive_words_{}.txt\".format(sentiment_dir, lang.lower()), \"r\") as fr:\n",
    "        sentiment_lexica[lang][\"pos\"] = fr.read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_fix = {\n",
    "    \"Bachiller D. P. Gatell\": \"Bachiller D. P. Gatell.\",\n",
    "    \"Eliza Haywood\": \"Eliza Fowler Haywood\",\n",
    "}\n",
    "texts_df[\"author\"] = texts_df[\"author\"].replace(author_fix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_df[\"language\"] = texts_df[\"language\"].replace(\"Spanish; Castilian\", \"Spanish\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_df[\"date\"] = texts_df[\"date\"].apply(lambda x: x.split(\"-\")[0])\n",
    "texts_df[\"date\"] = texts_df[\"date\"].apply(lambda x: x.split(\" [\")[0])\n",
    "texts_df[\"date\"] = texts_df[\"date\"].apply(lambda x: x.split(\" bzw.\")[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce to defined languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_df = texts_df[texts_df[\"language\"].isin(languages)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment(text, nl, pl):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    num_negative = 0\n",
    "    num_positive = 0\n",
    "    for nw in nl:\n",
    "        num_negative += tokens.count(nw.lower())\n",
    "    for pw in pl:\n",
    "        num_positive += tokens.count(pw.lower())\n",
    "    try:\n",
    "        score = (num_positive - num_negative) / (num_positive + num_negative)\n",
    "    except ZeroDivisionError:\n",
    "        score = 0\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_df[\"sentiment\"] = 0\n",
    "for language in languages:\n",
    "    lang_df = texts_df.loc[texts_df[\"language\"] == language]\n",
    "    neg_lexicon = sentiment_lexica[language][\"neg\"]\n",
    "    pos_lexicon = sentiment_lexica[language][\"pos\"]\n",
    "    scores = lang_df[\"text\"].progress_apply(analyze_sentiment, args=[neg_lexicon, pos_lexicon])\n",
    "    texts_df[\"sentiment\"].update(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in languages:\n",
    "    lang_df = texts_df.loc[texts_df[\"language\"] == language]\n",
    "    journal_group = lang_df.groupby(\"filename\")\n",
    "    authors = lang_df[\"author\"].unique()\n",
    "    num_authors = len(authors)\n",
    "    if \"Anonym\" in authors:\n",
    "        num_authors -= 1\n",
    "        num_anonymus = journal_group.apply(lambda x: 1 if all(x[\"author\"] == \"Anonym\") else 0).sum()\n",
    "    else:\n",
    "        num_anonymus = 0\n",
    "    topics = lang_df[\"topics\"].apply(lambda x:pd.Series(list(x))).reset_index().melt(id_vars=\"index\").dropna()[[\"index\", \"value\"]].set_index(\"index\")\n",
    "    years = lang_df[\"date\"].unique()\n",
    "        \n",
    "    print(language)\n",
    "    print(\"num authors:\",  num_authors)\n",
    "    print(\"num_anonymous:\", num_anonymus)\n",
    "    print(\"num journals:\", len(journal_group))\n",
    "    print(\"num text passages:\",  lang_df.shape[0])\n",
    "    print(\"num topics:\",  len(np.unique(topics)))\n",
    "    print(\"years:\", np.min(years), np.max(years))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in languages:\n",
    "    language_df = texts_df[texts_df[\"language\"] == language]\n",
    "    #print(language_df)\n",
    "    fig, ax = plt.subplots(figsize=(10,2.5))\n",
    "    sns.lineplot(data=language_df, x=\"date\", y=\"sentiment\", ax=ax)\n",
    "    plt.draw()\n",
    "    ax.set_xlabel(\"Years\")\n",
    "    ax.set_ylabel(\"Mean Sentiment\")\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Narrative forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in languages:\n",
    "    language_df = texts_df[texts_df[\"language\"] == language].copy()\n",
    "    \n",
    "    # standardize\n",
    "    language_df[\"sentiment\"] = language_df[\"sentiment\"] - language_df[\"sentiment\"].mean()\n",
    "    language_df[\"sentiment\"] = language_df[\"sentiment\"] / language_df[\"sentiment\"].std()\n",
    "    fig, ax = plt.subplots(figsize=(10,5))\n",
    "    sns.pointplot(data=language_df, x=\"ndf\", y=\"sentiment\", ax=ax, marker=\"s\", join=False)\n",
    "    plt.draw()\n",
    "    ax.set_xlabel(\"Narrative Form\")\n",
    "    ax.set_ylabel(\"Mean Standardized Sentiment\")\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = texts_df[\"topics\"].apply(lambda x:pd.Series(list(x))).reset_index().melt(id_vars=\"index\").dropna()[[\"index\", \"value\"]].set_index(\"index\")\n",
    "t_s_df = pd.merge(topics, texts_df[[\"sentiment\", \"language\"]], left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in languages:\n",
    "    language_df = t_s_df[t_s_df[\"language\"] == language].copy()\n",
    "    language_df[\"sentiment\"] = language_df[\"sentiment\"] - language_df[\"sentiment\"].mean()\n",
    "    language_df[\"sentiment\"] = language_df[\"sentiment\"] / language_df[\"sentiment\"].std()\n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    sns.pointplot(data=language_df, x=\"value\", y=\"sentiment\", ax=ax, marker=\"s\", join=False)\n",
    "    plt.draw()\n",
    "    ax.set_xlabel(\"Topic\")\n",
    "    ax.set_ylabel(\"Mean Standardized Sentiment\")\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Word Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(row):\n",
    "    lang = row[\"language\"]\n",
    "    if lang not in nlp_to_use.keys():\n",
    "        return \"\"\n",
    "    doc = nlp_to_use[lang](row[\"text\"])\n",
    "    tokens = []\n",
    "    for t in doc:\n",
    "        tokens.append(t.lemma_)\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "texts_df[\"tokens\"] = texts_df.progress_apply(lemmatize, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### Create graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occurrences = {}\n",
    "num_random_runs = 1000\n",
    "for language in languages:\n",
    "    language_df = texts_df[texts_df[\"language\"] == language]\n",
    "    stop_words = get_stop_words(language.lower())\n",
    "    vectorizer = CountVectorizer(max_df=0.8, stop_words=stop_words)\n",
    "    frequencies = vectorizer.fit_transform(language_df[\"tokens\"]).T\n",
    "    frequencies_df = pd.DataFrame(frequencies.todense(), index=vectorizer.get_feature_names())\n",
    "    frequencies_df = frequencies_df[frequencies_df.index.isin(list(set(sentiment_lexica[language][\"neg\"]) | set(sentiment_lexica[language][\"pos\"])))]\n",
    "    similarity_df = pd.DataFrame(cosine_similarity(frequencies_df), index=frequencies_df.index, columns=frequencies_df.index)\n",
    "    pairwise_df = similarity_df.where(np.triu(np.ones(similarity_df.shape)).astype(np.bool)).stack().reset_index()\n",
    "    pairwise_df.columns = [\"source\", \"target\", \"similarity\"]\n",
    "    pairwise_df[\"similarity\"] = pairwise_df[\"similarity\"].round(5)\n",
    "    p_values_df = pd.DataFrame()\n",
    "    p_values_df[\"source\"] = pairwise_df[\"source\"]\n",
    "    p_values_df[\"target\"] = pairwise_df[\"target\"]\n",
    "    p_values_df[\"p_value\"] = 0\n",
    "    for rr in tqdm(range(num_random_runs)):\n",
    "        random_frequencies_df = frequencies_df.sample(frac=1)\n",
    "        random_frequencies_df.index = frequencies_df.index\n",
    "        random_similarity_df = pd.DataFrame(cosine_similarity(random_frequencies_df), index=frequencies_df.index, columns=frequencies_df.index)\n",
    "        random_pairwise_df = random_similarity_df.where(np.triu(np.ones(random_similarity_df.shape)).astype(np.bool)).stack().reset_index()\n",
    "        random_pairwise_df.columns = [\"source\", \"target\", \"similarity\"]\n",
    "        random_pairwise_df[\"similarity\"] = random_pairwise_df[\"similarity\"].round(5)\n",
    "        p_values_df.loc[pairwise_df[random_pairwise_df[\"similarity\"] >= pairwise_df[\"similarity\"]].index, \"p_value\"] += 1\n",
    "    p_values_df[\"p_value\"] = (p_values_df[\"p_value\"] + 1) / (num_random_runs + 1)\n",
    "    occurrences[language] = p_values_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs = {}\n",
    "for language in languages:\n",
    "    graph = ig.Graph(directed=False)\n",
    "    significant_pair_df = occurrences[language][occurrences[language][\"p_value\"] < 0.01]\n",
    "    vertices = list(set(significant_pair_df[\"source\"]) | set(significant_pair_df[\"target\"]))\n",
    "    for vertex in vertices:\n",
    "        if vertex in sentiment_lexica[language][\"neg\"]:\n",
    "            graph.add_vertex(vertex, sent=1, color=\"red\", sentiment=\"negative\")\n",
    "        if vertex in sentiment_lexica[language][\"pos\"]:\n",
    "            graph.add_vertex(vertex, sent=2, color=\"green\", sentiment=\"positive\")\n",
    "    for idx, row in significant_pair_df.iterrows():\n",
    "        graph.add_edge(row[\"source\"], row[\"target\"], p_value=row[\"p_value\"])\n",
    "    \n",
    "    components = graph.clusters()\n",
    "    print(language)\n",
    "    for c in components:\n",
    "        print(len(c), end=\" \")\n",
    "    print()\n",
    "    \n",
    "    graphs[language] = components.giant()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results/graphs.p\", \"wb\") as handle:\n",
    "    pickle.dump(graphs, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results/graphs.p\", \"rb\") as handle:\n",
    "    graphs = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in languages:\n",
    "    num_nodes = len(graphs[language].vs)\n",
    "    num_positive_nodes = len(np.where(np.array(graphs[language].vs[\"sentiment\"]) == \"positive\")[0])\n",
    "    num_negative_nodes = len(np.where(np.array(graphs[language].vs[\"sentiment\"]) == \"negative\")[0])\n",
    "    print(language)\n",
    "    print(\"num nodes:\", num_nodes)\n",
    "    print(\"num positive nodes:\", num_positive_nodes, round(num_positive_nodes/num_nodes, 2))\n",
    "    print(\"num negative nodes:\", num_negative_nodes, round(num_negative_nodes/num_nodes, 2))\n",
    "    print(\"num edges:\", len(graphs[language].es))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot French graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_style = {}\n",
    "visual_style[\"vertex_size\"] = 20\n",
    "visual_style[\"vertex_label\"] = graphs[\"French\"].vs[\"name\"]\n",
    "visual_style[\"vertex_label_size\"] = 15\n",
    "visual_style[\"layout\"] = graphs[\"French\"].layout_fruchterman_reingold()#layout_kamada_kawai()\n",
    "visual_style[\"bbox\"] = (1400, 1000)\n",
    "visual_style[\"margin\"] = 50\n",
    "ig.plot(graphs[\"French\"], \"results/plots/sentiment_word_network_french.pdf\", **visual_style)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot German graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_style = {}\n",
    "visual_style[\"vertex_size\"] = 20\n",
    "visual_style[\"vertex_label\"] = graphs[\"German\"].vs[\"name\"]\n",
    "visual_style[\"vertex_label_size\"] = 15\n",
    "visual_style[\"layout\"] = graphs[\"German\"].layout_fruchterman_reingold(niter=100000)#layout_kamada_kawai()\n",
    "visual_style[\"bbox\"] = (1400, 1000)\n",
    "visual_style[\"margin\"] = 50\n",
    "ig.plot(graphs[\"German\"], \"results/plots/sentiment_word_network_german.pdf\", **visual_style)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Portuguese graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_style = {}\n",
    "visual_style[\"vertex_size\"] = 20\n",
    "visual_style[\"vertex_label\"] = graphs[\"Portuguese\"].vs[\"name\"]\n",
    "visual_style[\"vertex_label_size\"] = 15\n",
    "visual_style[\"layout\"] = graphs[\"Portuguese\"].layout_fruchterman_reingold(niter=100000)#layout_kamada_kawai()\n",
    "visual_style[\"bbox\"] = (1400, 1000)\n",
    "visual_style[\"margin\"] = 50\n",
    "ig.plot(graphs[\"Portuguese\"], \"results/plots/sentiment_word_network_portuguese.pdf\", **visual_style)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most significant edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_sig_list = []\n",
    "for language in languages:\n",
    "    edges_list = []\n",
    "    for e in graphs[language].es:\n",
    "        edges_list.append({\"source\": graphs[language].vs[e.source][\"name\"], \"target\": graphs[language].vs[e.target][\"name\"], \"p_value\": e[\"p_value\"]})\n",
    "    edges_df = pd.DataFrame(edges_list)\n",
    "    top_sig_s = edges_df.sort_values(\"p_value\").head(10).reset_index().apply(lambda x: \"{} - {}\".format(x[\"source\"], x[\"target\"]), axis=1)\n",
    "    top_sig_s.name = language\n",
    "    top_sig_list.append(top_sig_s)\n",
    "top_sig_df = pd.concat(top_sig_list, axis=1)\n",
    "print(top_sig_df.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_results = {}\n",
    "centrality_results_df = pd.DataFrame()\n",
    "lcc_results = {}\n",
    "assortativity_results = {}\n",
    "for language, graph in graphs.items():\n",
    "    # dict setup\n",
    "    degree_results[language] = {}\n",
    "    lcc_results[language] = {}\n",
    "    assortativity_results[language] = {}\n",
    "    \n",
    "    # degree\n",
    "    degree_results[language][\"all\"] = graph.degree()\n",
    "    degree_results[language][\"neg\"] = graph.degree(np.where(np.array(graph.vs[\"sentiment\"]) == \"negative\")[0])\n",
    "    degree_results[language][\"pos\"] = graph.degree(np.where(np.array(graph.vs[\"sentiment\"]) == \"positive\")[0])\n",
    "    \n",
    "    # centralities\n",
    "    centrality_df = pd.DataFrame()\n",
    "    centrality_df[\"word\"] = graph.vs[\"name\"]\n",
    "    centrality_df[\"sentiment\"] = graph.vs[\"sentiment\"]\n",
    "    centrality_df[\"degree\"] = graph.degree()\n",
    "    centrality_df[\"betweenness\"] = graph.betweenness(directed=False)\n",
    "    centrality_df[\"closeness\"] = graph.closeness()\n",
    "    centrality_df[\"language\"] = language\n",
    "    centrality_results_df = centrality_results_df.append(centrality_df)\n",
    "    \n",
    "    # clustering coefficient\n",
    "    lcc_results[language][\"all\"] = graph.transitivity_local_undirected(mode=\"0\")\n",
    "    lcc_results[language][\"neg\"] = graph.transitivity_local_undirected(np.where(np.array(graph.vs[\"sentiment\"]) == \"negative\")[0], mode=\"0\")\n",
    "    lcc_results[language][\"pos\"] = graph.transitivity_local_undirected(np.where(np.array(graph.vs[\"sentiment\"]) == \"positive\")[0], mode=\"0\")\n",
    "    \n",
    "    # assortativity\n",
    "    assortativity_results[language][\"degree\"] = graph.assortativity_degree(directed=False)\n",
    "    assortativity_results[language][\"sentiment\"] = graph.assortativity(\"sent\", directed=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CDF Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in languages:\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    \n",
    "    degree_all = degree_results[language][\"all\"]\n",
    "    degree_neg = degree_results[language][\"neg\"]\n",
    "    degree_pos = degree_results[language][\"pos\"]\n",
    "    \n",
    "    sns.kdeplot(degree_all, color=\"black\", cumulative=True, ax=ax)\n",
    "    sns.kdeplot(degree_neg, color=\"red\", cumulative=True, ax=ax)\n",
    "    sns.kdeplot(degree_pos, color=\"green\", cumulative=True, ax=ax)\n",
    "    \n",
    "    ax.set_ylabel(\"CDF\")\n",
    "    ax.set_xlabel(\"Degree\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    #plt.show()\n",
    "    plt.savefig(\"results/plots/cdf_degree_{}.pdf\".format(language))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KS tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in languages:\n",
    "    degree_neg = degree_results[language][\"neg\"]\n",
    "    degree_pos = degree_results[language][\"pos\"]\n",
    "    \n",
    "    print(language)\n",
    "    print(ks_2samp(degree_neg, degree_pos))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Centralities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Positive/negtive ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in languages:\n",
    "    deg_top_words = centrality_results_df[centrality_results_df[\"language\"] == language].sort_values(\"degree\", ascending=False).head(50)\n",
    "    bet_top_words = centrality_results_df[centrality_results_df[\"language\"] == language].sort_values(\"betweenness\", ascending=False).head(50)\n",
    "    clo_top_words = centrality_results_df[centrality_results_df[\"language\"] == language].sort_values(\"closeness\", ascending=False).head(50)\n",
    "    \n",
    "    print(language)\n",
    "    print(\"degree\")\n",
    "    print(deg_top_words[\"sentiment\"].value_counts() / 50)\n",
    "    print(\"betweenness\")\n",
    "    print(bet_top_words[\"sentiment\"].value_counts() / 50)\n",
    "    print(\"closeness\")\n",
    "    print(clo_top_words[\"sentiment\"].value_counts() / 50)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wordclouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in tqdm(languages):\n",
    "    def color_words(word, font_size, position, orientation, random_state, font_path):\n",
    "        return graphs[language].vs[graphs[language].vs[\"name\"].index(word)][\"color\"]\n",
    "\n",
    "    deg_top_words = centrality_results_df[centrality_results_df[\"language\"] == language].sort_values(\"degree\", ascending=False).head(50)\n",
    "    deg_top_words_dict = deg_top_words.set_index(\"word\")[\"degree\"].to_dict()\n",
    "    \n",
    "    bet_top_words = centrality_results_df[centrality_results_df[\"language\"] == language].sort_values(\"betweenness\", ascending=False).head(50)\n",
    "    bet_top_words_dict = bet_top_words.set_index(\"word\")[\"betweenness\"].to_dict()\n",
    "    \n",
    "    clo_top_words = centrality_results_df[centrality_results_df[\"language\"] == language].sort_values(\"closeness\", ascending=False).head(50)\n",
    "    clo_top_words_dict = clo_top_words.set_index(\"word\")[\"closeness\"].to_dict()\n",
    "    \n",
    "    deg_wordcloud = WordCloud(font_path=\"arial.ttf\", width=2000,height=1000, background_color=\"white\", color_func=color_words).generate_from_frequencies(deg_top_words_dict)\n",
    "    bet_wordcloud = WordCloud(font_path=\"arial.ttf\", width=2000,height=1000, background_color=\"white\", color_func=color_words).generate_from_frequencies(bet_top_words_dict)\n",
    "    clo_wordcloud = WordCloud(font_path=\"arial.ttf\", width=2000,height=1000, background_color=\"white\", color_func=color_words).generate_from_frequencies(clo_top_words_dict)\n",
    "    \n",
    "    with open(\"results/plots/wordcloud_degree_{}.svg\".format(language), \"w\") as svg_file:\n",
    "        svg_file.write(deg_wordcloud.to_svg())\n",
    "        \n",
    "    with open(\"results/plots/wordcloud_betweenness_{}.svg\".format(language), \"w\") as svg_file:\n",
    "        svg_file.write(bet_wordcloud.to_svg())\n",
    "        \n",
    "    with open(\"results/plots/wordcloud_closeness_{}.svg\".format(language), \"w\") as svg_file:\n",
    "        svg_file.write(clo_wordcloud.to_svg())\n",
    "        \n",
    "    deg_drawing = svg2rlg(\"results/plots/wordcloud_degree_{}.svg\".format(language))\n",
    "    renderPDF.drawToFile(deg_drawing, \"results/plots/wordcloud_degree_{}.pdf\".format(language))\n",
    "    \n",
    "    bet_drawing = svg2rlg(\"results/plots/wordcloud_betweenness_{}.svg\".format(language))\n",
    "    renderPDF.drawToFile(bet_drawing, \"results/plots/wordcloud_betweenness_{}.pdf\".format(language))\n",
    "    \n",
    "    clo_drawing = svg2rlg(\"results/plots/wordcloud_closeness_{}.svg\".format(language))\n",
    "    renderPDF.drawToFile(clo_drawing, \"results/plots/wordcloud_closeness_{}.pdf\".format(language))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proportion tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_smaller_sig = 0\n",
    "num_larger_sig = 0\n",
    "num_smaller_non_sig = 0 \n",
    "num_larger_non_sig = 0\n",
    "for centrality in [\"degree\", \"betweenness\", \"closeness\"]:\n",
    "    for language in languages:\n",
    "        language_df = centrality_results_df[centrality_results_df[\"language\"] == language]\n",
    "        temp_centrality_df =  language_df.sort_values(centrality, ascending=False).head(50)[\"sentiment\"].to_frame()\n",
    "        neg_count = temp_centrality_df[\"sentiment\"].value_counts()[\"negative\"]\n",
    "        neg_net_ratio = neg_count / 50\n",
    "        neg_graph_ratio = len(np.where(np.array(graphs[language].vs[\"sentiment\"]) == \"negative\")[0]) / graphs[language].vcount()\n",
    "        if neg_net_ratio < neg_graph_ratio:\n",
    "            pvalue = proportions_ztest(count=neg_count, nobs=100, value=neg_graph_ratio, alternative=\"smaller\")[1]\n",
    "            alternative = \"smaller\"\n",
    "        else:\n",
    "            pvalue = proportions_ztest(count=neg_count, nobs=100, value=neg_graph_ratio, alternative=\"larger\")[1]\n",
    "            alternative = \"larger\"\n",
    "            \n",
    "        if pvalue < 0.05:\n",
    "            sig = \"significant\"\n",
    "        else:\n",
    "            sig = \"non-significant\"\n",
    "            \n",
    "        if alternative == \"smaller\" and sig == \"significant\": num_smaller_sig += 1\n",
    "        if alternative == \"larger\" and sig == \"significant\": num_larger_sig += 1\n",
    "        if alternative == \"smaller\" and sig == \"non-significant\": num_smaller_non_sig += 1\n",
    "        if alternative == \"larger\" and sig == \"non-significant\": num_larger_non_sig += 1\n",
    "            \n",
    "        print(centrality, language, neg_net_ratio, round(neg_graph_ratio, 2), alternative, pvalue)\n",
    "print()\n",
    "print(num_smaller_sig)\n",
    "print(num_larger_sig)\n",
    "print(num_smaller_non_sig)\n",
    "print(num_larger_non_sig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in tqdm(languages):\n",
    "    language_df = centrality_results_df[centrality_results_df[\"language\"] == language][[\"degree\", \"betweenness\", \"closeness\"]]\n",
    "    \n",
    "    language_df.columns = [\"Degree\", \"Betweenness\", \"Closeness\"]\n",
    "    \n",
    "    sns.pairplot(language_df, diag_kind =\"kde\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"results/plots/centrality_corr_{}.pdf\".format(language))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in languages:\n",
    "    language_df = centrality_results_df[centrality_results_df[\"language\"] == language]\n",
    "    \n",
    "    print(language)\n",
    "    print(\"deg2bet\", spearmanr(language_df[\"degree\"], language_df[\"betweenness\"]))\n",
    "    print(\"deg2clo\", spearmanr(language_df[\"degree\"], language_df[\"closeness\"]))\n",
    "    print(\"bet2clo\", spearmanr(language_df[\"betweenness\"], language_df[\"closeness\"]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local clustering coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CDF plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in languages:\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    \n",
    "    lcc_all = lcc_results[language][\"all\"]\n",
    "    lcc_neg = lcc_results[language][\"neg\"]\n",
    "    lcc_pos = lcc_results[language][\"pos\"]\n",
    "    \n",
    "    sns.kdeplot(lcc_all, color=\"black\", cumulative=True, ax=ax)\n",
    "    sns.kdeplot(lcc_neg, color=\"red\", cumulative=True, ax=ax)\n",
    "    sns.kdeplot(lcc_pos, color=\"green\", cumulative=True, ax=ax)\n",
    "    \n",
    "    ax.set_ylabel(\"CDF\")\n",
    "    ax.set_xlabel(\"Local Clustering Coefficient\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    #plt.show()\n",
    "    plt.savefig(\"results/plots/cdf_lcc_{}.pdf\".format(language))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean and median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in languages:\n",
    "    lcc_all = lcc_results[language][\"all\"]\n",
    "    lcc_neg = lcc_results[language][\"neg\"]\n",
    "    lcc_pos = lcc_results[language][\"pos\"]\n",
    "    \n",
    "    print(language)\n",
    "    print(\"all mean:\", np.mean(lcc_all))\n",
    "    print(\"all median:\", np.median(lcc_all))\n",
    "    print(\"negative mean:\", np.mean(lcc_neg))\n",
    "    print(\"negative median:\", np.median(lcc_neg))\n",
    "    print(\"positive mean:\", np.mean(lcc_pos))\n",
    "    print(\"positive median:\", np.median(lcc_pos))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KS tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in languages:\n",
    "    lcc_neg = lcc_results[language][\"neg\"]\n",
    "    lcc_pos = lcc_results[language][\"pos\"]\n",
    "    \n",
    "    print(language)\n",
    "    print(ks_2samp(lcc_neg, lcc_pos))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assortativity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in languages:\n",
    "    print(language)\n",
    "    print(\"degree assortativity:\", assortativity_results[language][\"degree\"])\n",
    "    print(\"sentiment assortativity:\", assortativity_results[language][\"sentiment\"])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
