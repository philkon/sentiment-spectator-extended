{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import igraph as ig\n",
    "import nltk\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from stop_words import get_stop_words\n",
    "from scipy.stats import spearmanr, ks_2samp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_input_path = \"data/texts.p\"\n",
    "sentiment_dir = \"data/sentiment/\" # use / at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "plt.rc('ps',fonttype = 42)\n",
    "plt.rc('pdf',fonttype = 42)\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "plt.rcParams['ps.useafm'] = True\n",
    "plt.rcParams['pdf.use14corefonts'] = True\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = [\"French\", \"German\", \"Italian\", \"Portuguese\", \"Spanish\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_df = pd.read_pickle(dataframe_input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_nlp = sc.load(\"de_core_news_sm\")\n",
    "fr_nlp = sc.load(\"fr_core_news_sm\")\n",
    "es_nlp = sc.load(\"es_core_news_sm\")\n",
    "it_nlp = sc.load(\"it_core_news_sm\")\n",
    "pt_nlp = sc.load(\"pt_core_news_sm\")\n",
    "\n",
    "nlp_to_use = {\n",
    "    \"French\": fr_nlp,\n",
    "    \"German\": de_nlp,\n",
    "    \"Italian\": it_nlp,\n",
    "    \"Portuguese\": pt_nlp,\n",
    "    \"Spanish\": es_nlp\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_lexica = {}\n",
    "for lang in languages:\n",
    "    sentiment_lexica[lang] = {}\n",
    "    with open(\"{}negative_words_{}.txt\".format(sentiment_dir, lang.lower()), \"r\") as fr:\n",
    "        sentiment_lexica[lang][\"neg\"] = fr.read().splitlines()\n",
    "    with open(\"{}positive_words_{}.txt\".format(sentiment_dir, lang.lower()), \"r\") as fr:\n",
    "        sentiment_lexica[lang][\"pos\"] = fr.read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_fix = {\n",
    "    \"Bachiller D. P. Gatell\": \"Bachiller D. P. Gatell.\",\n",
    "    \"Eliza Haywood\": \"Eliza Fowler Haywood\",\n",
    "}\n",
    "texts_df[\"author\"] = texts_df[\"author\"].replace(author_fix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_df[\"language\"] = texts_df[\"language\"].replace(\"Spanish; Castilian\", \"Spanish\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_df[\"date\"] = texts_df[\"date\"].apply(lambda x: x.split(\"-\")[0])\n",
    "texts_df[\"date\"] = texts_df[\"date\"].apply(lambda x: x.split(\" [\")[0])\n",
    "texts_df[\"date\"] = texts_df[\"date\"].apply(lambda x: x.split(\" bzw.\")[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce to defined languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_df = texts_df[texts_df[\"language\"].isin(languages)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment(text, nl, pl):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    num_negative = 0\n",
    "    num_positive = 0\n",
    "    for nw in nl:\n",
    "        num_negative += tokens.count(nw.lower())\n",
    "    for pw in pl:\n",
    "        num_positive += tokens.count(pw.lower())\n",
    "    try:\n",
    "        score = (num_positive - num_negative) / (num_positive + num_negative)\n",
    "    except ZeroDivisionError:\n",
    "        score = 0\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_df[\"sentiment\"] = 0\n",
    "for language in languages:\n",
    "    lang_df = texts_df.loc[texts_df[\"language\"] == language]\n",
    "    neg_lexicon = sentiment_lexica[language][\"neg\"]\n",
    "    pos_lexicon = sentiment_lexica[language][\"pos\"]\n",
    "    scores = lang_df[\"text\"].progress_apply(analyze_sentiment, args=[neg_lexicon, pos_lexicon])\n",
    "    texts_df[\"sentiment\"].update(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in languages:\n",
    "    lang_df = texts_df.loc[texts_df[\"language\"] == language]\n",
    "    journal_group = lang_df.groupby(\"filename\")\n",
    "    authors = lang_df[\"author\"].unique()\n",
    "    num_authors = len(authors)\n",
    "    if \"Anonym\" in authors:\n",
    "        num_authors -= 1\n",
    "        num_anonymus = journal_group.apply(lambda x: 1 if all(x[\"author\"] == \"Anonym\") else 0).sum()\n",
    "    else:\n",
    "        num_anonymus = 0\n",
    "    topics = lang_df[\"topics\"].apply(lambda x:pd.Series(list(x))).reset_index().melt(id_vars=\"index\").dropna()[[\"index\", \"value\"]].set_index(\"index\")\n",
    "    years = lang_df[\"date\"].unique()\n",
    "        \n",
    "    print(language)\n",
    "    print(\"num authors:\",  num_authors)\n",
    "    print(\"num_anonymous:\", num_anonymus)\n",
    "    print(\"num journals:\", len(journal_group))\n",
    "    print(\"num text passages:\",  lang_df.shape[0])\n",
    "    print(\"num topics:\",  len(np.unique(topics)))\n",
    "    print(\"years:\", np.min(years), np.max(years))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_df[\"sentiment\"] = texts_df[\"sentiment\"].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in languages:\n",
    "    language_df = texts_df[texts_df[\"language\"] == language]\n",
    "    #print(language_df)\n",
    "    fig, ax = plt.subplots(figsize=(10,2.5))\n",
    "    sns.lineplot(data=language_df, x=\"date\", y=\"sentiment\", ax=ax)\n",
    "    plt.draw()\n",
    "    ax.set_xlabel(\"Years\")\n",
    "    ax.set_ylabel(\"Mean Sentiment\")\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Narrative forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in languages:\n",
    "    language_df = texts_df[texts_df[\"language\"] == language].copy()\n",
    "    \n",
    "    # standardize\n",
    "    language_df[\"sentiment\"] = language_df[\"sentiment\"] - language_df[\"sentiment\"].mean()\n",
    "    language_df[\"sentiment\"] = language_df[\"sentiment\"] / language_df[\"sentiment\"].std()\n",
    "    fig, ax = plt.subplots(figsize=(10,5))\n",
    "    sns.pointplot(data=language_df, x=\"ndf\", y=\"sentiment\", ax=ax, marker=\"s\", join=False)\n",
    "    plt.draw()\n",
    "    ax.set_xlabel(\"Narrative Form\")\n",
    "    ax.set_ylabel(\"Mean Standardized Sentiment\")\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = texts_df[\"topics\"].apply(lambda x:pd.Series(list(x))).reset_index().melt(id_vars=\"index\").dropna()[[\"index\", \"value\"]].set_index(\"index\")\n",
    "t_s_df = pd.merge(topics, texts_df[[\"sentiment\", \"language\"]], left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in languages:\n",
    "    language_df = t_s_df[t_s_df[\"language\"] == language].copy()\n",
    "    language_df[\"sentiment\"] = language_df[\"sentiment\"] - language_df[\"sentiment\"].mean()\n",
    "    language_df[\"sentiment\"] = language_df[\"sentiment\"] / language_df[\"sentiment\"].std()\n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    sns.pointplot(data=language_df, x=\"value\", y=\"sentiment\", ax=ax, marker=\"s\", join=False)\n",
    "    plt.draw()\n",
    "    ax.set_xlabel(\"Topic\")\n",
    "    ax.set_ylabel(\"Mean Standardized Sentiment\")\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Word Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(row):\n",
    "    lang = row[\"language\"]\n",
    "    if lang not in nlp_to_use.keys():\n",
    "        return \"\"\n",
    "    doc = nlp_to_use[lang](row[\"text\"])\n",
    "    tokens = []\n",
    "    for t in doc:\n",
    "        tokens.append(t.lemma_)\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "texts_df[\"tokens\"] = texts_df.progress_apply(lemmatize, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs = {}\n",
    "for language in languages:\n",
    "    language_df = texts_df[texts_df[\"language\"] == language]\n",
    "    vectorizer = CountVectorizer(max_df=0.9)\n",
    "    frequencies = vectorizer.fit_transform(language_df[\"tokens\"])\n",
    "    frequencies_df = pd.DataFrame(frequencies, columns=vectorizer.get_feature_names())\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_results = {}\n",
    "centrality_results_df = pd.DataFrame()\n",
    "lcc_results = {}\n",
    "assortativity_results = {}\n",
    "for language, graph in graphs.items():\n",
    "    # degree\n",
    "    degree_results[language][\"all\"] = graph.degree()\n",
    "    degree_results[language][\"neg\"] = graph.degree(np.where(np.array(graph.vs[\"sentiment\"]) == \"negative\")[0])\n",
    "    degree_results[language][\"pos\"] = graph.degree(np.where(np.array(graph.vs[\"sentiment\"]) == \"positive\")[0])\n",
    "    \n",
    "    # centralities\n",
    "    centrality_df = pd.DataFrame()\n",
    "    centrality_df[\"word\"] = graph.vs[\"name\"]\n",
    "    centrality_df[\"degree\"] = graph.degree()\n",
    "    centrality_df[\"betweenness\"] = graph.betweenness(directed=False)\n",
    "    centrality_df[\"closeness\"] = graph.closeness()\n",
    "    centrality_df[\"language\"] = language\n",
    "    centrality_results_df = centrality_results_df.append(centrality_df)\n",
    "    \n",
    "    # clustering coefficient\n",
    "    lcc_results[language][\"all\"] = graph.transitivity_local_undirected()\n",
    "    lcc_results[language][\"neg\"] = graph.transitivity_local_undirected(np.where(np.array(graph.vs[\"sentiment\"]) == \"negative\")[0])\n",
    "    lcc_results[language][\"pos\"] = graph.transitivity_local_undirected(np.where(np.array(graph.vs[\"sentiment\"]) == \"positive\")[0])\n",
    "        \n",
    "    # assortativity\n",
    "    assortativity_results[language][\"degree\"] = graph.assortativity_degree(directed=False)\n",
    "    assortativity_results[language][\"sentiment\"] = graph.assortativity(\"sent\", directed=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CDF Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in languages:\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    \n",
    "    degree_all = degree_results[language][\"all\"]\n",
    "    degree_neg = degree_results[language][\"neg\"]\n",
    "    degree_pos = degree_results[language][\"pos\"]\n",
    "    \n",
    "    sns.kdeplot(degree_all, color=\"black\", cumulative=True, ax=ax)\n",
    "    sns.kdeplot(degree_neg, color=\"red\", cumulative=True, ax=ax)\n",
    "    sns.kdeplot(degree_pos, color=\"green\", cumulative=True, ax=ax)\n",
    "    \n",
    "    ax.set_ylabel(\"CDF\")\n",
    "    ax.set_xlabel(\"Degree\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KS tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in languages:\n",
    "    degree_neg = degree_results[language][\"neg\"]\n",
    "    degree_pos = degree_results[language][\"pos\"]\n",
    "    \n",
    "    print(language)\n",
    "    print(ks_2samp(degree_neg, degree_pos))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Centralities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local clustering coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean and median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in languages:\n",
    "    lcc_all = lcc_results[language][\"all\"]\n",
    "    lcc_neg = lcc_results[language][\"neg\"]\n",
    "    lcc_pos = lcc_results[language][\"pos\"]\n",
    "    \n",
    "    print(language)\n",
    "    print(\"all mean:\", np.mean(lcc_all))\n",
    "    print(\"all median:\", np.median(lcc_all))\n",
    "    print(\"negative mean:\", np.mean(lcc_neg))\n",
    "    print(\"negative median:\", np.median(lcc_neg))\n",
    "    print(\"positive mean:\", np.mean(lcc_pos))\n",
    "    print(\"positive median:\", np.median(lcc_pos))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KS tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in languages:\n",
    "    lcc_neg = lcc_results[language][\"neg\"]\n",
    "    lcc_pos = lcc_results[language][\"pos\"]\n",
    "    \n",
    "    print(language)\n",
    "    print(ks_2samp(lcc_neg, lcc_pos))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assortativity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in languages:\n",
    "    print(language)\n",
    "    print(\"degree assortativity:\", assortativity_results[language][\"degree\"])\n",
    "    print(\"sentiment assortativity:\", assortativity_results[language][\"sentiment\"])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
